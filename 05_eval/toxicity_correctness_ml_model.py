"""
Module: Toxicity and Factual Correctness Evaluation
This module evaluates the toxicity of text responses using a BERT-based classifier
and measures factual consistency using cosine similarity between embeddings.
"""

# -----------------------------------------------------------------------------
# Import Dependencies
# -----------------------------------------------------------------------------
from transformers import pipeline

# -----------------------------------------------------------------------------
# Initialize Toxicity Detector
# -----------------------------------------------------------------------------
# Load a pre-trained toxicity classifier model ("unitary/toxic-bert").
toxicity_detector = pipeline("text-classification", model="unitary/toxic-bert")


# -----------------------------------------------------------------------------
# Function: check_toxicity
# -----------------------------------------------------------------------------
def check_toxicity(response):
    """
    Checks if a response contains toxic or offensive language and returns a toxicity score.

    Args:
        response (str): The text response to evaluate.

    Returns:
        float: Toxicity score (higher indicates more toxicity).
    """
    # Get the toxicity score from the model output.
    toxicity_score = toxicity_detector(response)[0]["score"]
    return toxicity_score


# -----------------------------------------------------------------------------
# Function: check_factual_correctness
# -----------------------------------------------------------------------------
def check_factual_correctness(response, reference_text):
    """
    Measures factual consistency between the AI response and a known reference.
    Uses cosine similarity between text embeddings generated by a pre-trained model.

    Args:
        response (str): The AI-generated response.
        reference_text (str): The reference text for comparison.

    Returns:
        float: Cosine similarity score (1 indicates identical content).
    """
    # Import required modules from SentenceTransformers.
    from sentence_transformers import SentenceTransformer, util

    # Load a pre-trained model for generating text embeddings.
    model = SentenceTransformer("all-MiniLM-L6-v2")
    # Generate embeddings for the response and reference text.
    response_embedding = model.encode(response, convert_to_tensor=True)
    reference_embedding = model.encode(reference_text, convert_to_tensor=True)

    # Calculate the cosine similarity between the two embeddings.
    similarity_score = util.pytorch_cos_sim(
        response_embedding, reference_embedding
    ).item()
    return similarity_score


# -----------------------------------------------------------------------------
# Example Usage
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    # Sample text for evaluation
    response_text = "The Earth circles the Sun."
    reference_text = "The Earth orbits around the sun."

    # Evaluate toxicity and factual correctness
    toxicity_score = check_toxicity(response_text)
    correctness_score = check_factual_correctness(response_text, reference_text)

    # Print the evaluation scores
    print(f"Toxicity Score: {toxicity_score}")
    print(f"Correctness Score: {correctness_score}")